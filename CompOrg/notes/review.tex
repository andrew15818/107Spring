\documentclass{article}
\usepackage[margin=0.5in]{geometry}
\title{Computer Organization Notes}
\begin{document}
\section{3.6 Parallelism and Computer Arithmetic: Subword Parallelism}
As transistors became more available and accessible, we naturally became more interested in developing 
new functionality for computers. The ability to process other types of data such as video, graphics and
audio, we would need more than 32 bits to interpret. With the new instruction sets, we can operate on
different quantities of bits within a word. For example, traditionally colors and graphics  was represented
with 8 bits. Then, combined with sound options, which we can represent with 16 bits.

For example, the new ARM architecture allows 256 bytes worth of new registers. We can view them as 8 byte
registers, in which case we have 32, or 16 16-bit registers. This ability to perform multiple operations at the 
same time is called \textbf{subword parallelism}.
\section{3.9 Fallacies and Pitfalls}
\subsection{Since shifting left multiplies by two, shifting right divided by two}

This is not always the case. When we shift in values to the most significant bits, we usually will shift 0s.
This might happen when the sign bit is negative, i.e. when we are shifting in a positive sign bit for a number that
is negative. However, even if we try to extend the sing bit, it might lead us to smaller errors but errors nonetheless.
\subsection{Floating-point addition is not associative}
Floating-point additition is not associative, especially when we add two large numbers with a small one. Because the
scientific might not change when we add a small number, if we add a small number it might not make much of a difference.
\subsection{Parallel execution strategies that work for integer data types also work for floating-point data types}
Because integer addition is associative and floating-point addition is not associative. Thus when we rewrite a 
program to run in parallel instead of sequentially, the answer might not be the same.

Thus prgrams even with the same inputs might not produce the same results as code that was run in parallel.
\subsection{THe MIPS instruction add immediate unsigned \texttt{addiu} sign-extends the 16-bit immediate field}
Because this instruction is used for adding unsigned integers,  it really is not that necessary to sign-extend the
number.
\subsection{Only  theoretical mathematicians care about floating-point accuracy}
The Intel Pentium in 1994 had a buf where some of the bits in floating point arithmetic would be wrong occasionally.
What would happen is that floating point division would sometimes return a wrong bits. This was due to incorrect
lookups in the programmable logic array. Thus, values that were supposed to return a +2 would return a 0. In using 
the SRT algorithm for division, it would use a lookup table based on the dividend and divisor. This badly programmed
lookup table is what caused the incorrect values to be retrieved. Only a few of the total entries were even faulty,
5/1066 entries.
\textbf{The concept of the stored program indicates that all bit patterns are essentially meaningless, and can mean 
many different things depending on how they are interpreted.}
\newpage
\section{Chapter 4: The Processor}
\subsection{4.1 Introduction}
All the different types of commands that we have looked at do essentially the same things. We first have to fetch
the instruction from memory and load it. Then, we have to decode the instruction and perform the corresponding
operation. Then, for all instructions (exept jump), we have to use the main ALU. Depending on the opcode and
function code, we can trigger a specific operation to occur. If the command is not a branch or jump instruction,
we incremement the PC by 4 to go to the next instruction.  In our diagram, we also have a \textit{control unit},
whcich determines the output to the control lines of various parts of our CPU, such as multiplexors and ALU.

The control unit takes the instruction as input.
\section{4.2 Logic Design Conventions}
To review: when we are talking about circuits, we can talk about either \textit{combinational} and 
\textit{sequential} circuits. The difference is that the \textit{combinational} circuits depend
only on the current input, whereas \textit{state} circuits or sequential circuits ahve some sort of memory 
component and an use it as an input to the next instruction.

An example of one of these circuits is a \textit{D-flip-flop}, where the inputs are just the value to be written
and the clock. Why do we have the clock as input? It is essentailly because we only want these operations to 
happen when the clock becomes positive. The output at any given time is the previous value.
\subsection{CLocking Methodology}
A clocking methodology refers to the period of time during an operation where we can read and write. If we 
could read and write data at the same time, then the output might be unpredictable.

Thus an \textbf{\textit{edge-triggered methodology}} would allow us to write a value to a register, send it 
through a combinational circuit, and receive the output all in the same cycle. The inputs for the logic 
elements are mostly 32-bits, since that is the size of inputs that our processor accepts.
\section{4.3 Building a Datapath}
The way to determine the next instruction is by using the PC, and use an ALU(which only performs add) to 
find the next instruction. Then, we need to select which operation with a multiplexer, because if it is 
a compare insrtuction, we mgiht have to select that, else we just increment the PC by 4.

For R-type instructions, we have a \textit{register file}, a state element which contains the 32 general 
purpose registers. We then just specify the number of the register to operate on. We will need an ALU to 
work on it though. The R-type instructions require that we work on 3 registers (read 2 and write to 1). We
then need to pass them all as inputs to the register file. The values of the registers that were read are 
then the outputs of the register file. These values are then presumably passed to the main ALU to be operated on.

Since the write control signal is edge triggered, will it be triggered automatically by the main control unit?
It would make sense, since all R-type instructions require a write to happen at the end of the instruction.


For a load instruction of the type \texttt{lw \$t1, offset\_value(\$t2) }, we need to add the signed field of the
offset value to the base register, \$t2 in this case, to the offset field in the intruction. For loads, we need
to then move the instruction into the register specified in the first instruction. When we move some data into
a register, we then have to sign extend, or copy the most significant bit in the field all the way to the end
of the register.

Then we move on to \texttt{beq}. Again, these instructions have three arguments. There are the two registers
that need to be compared for equality. The offset in the field is then added (or subtraced, since it is signed) 
to the address of the current branch instruction. This new address to jump to is called the \textit{branch 
target address}.
 
We need to remember that since we are using the PC as the base address, we are actually using the current PC+4
in our calculations, since we perform that addition in the fetch part of the cycle. Using the ALU, we need 
to check for the equality of the two registers.
\subsection{ Creating a Single Datapath}
Essentially to combine all the different segments we have needed, we use \textit{multiplexors} to select only 
one output depending on a certain input. We would use multiplexors to select just one input for the ALU,
for example to implement the R-type instructions. More specifically, whenever we had to select from 
multiple inputs, such as memory operands and values decoded from the instruction, we would use a multiplexor
to feed the input to the ALU.
\section{4.4 A Simple Implementation Scheme}
How do we select the values to be used by the ALU? It depends on the function field of the instruction. Because
we need to determien the operation to be performed by the ALU, we would need a separate \textit{control unit}.
The job of the control unit is to select one operation to be performed by the ALU. We use 2 bits for the ALUOP,
which we use to tell the control unit if we want to perform a branch, R-type or load/store instruction.

Based on that info, we then (in the case of R-Type operations add,sub,and, or,slt) would need the remaining
6-bit \textit{funct} field found in the instruction. We map the combination of bits to the ALU control input, 
which will go directly to the register. To turn the inputs into gates, we can construct a truth table for the 
inputs. Combining the opcode and funct fields give us a total of 
\begin{equation}
		2^{8} 
\end{equation}
possible inputs. However, most of these inputs are implemented as \textit{don't care} conditions, so we don't 
take them into consideration.

When parsing or decoding the opcode, we fortunately look at the same format regardless of instruciton. The first
4 bits of all different instruction types are the opcode, so we do not really have to imiplement more hardware
becuase \textit{simplicity favors regularity} :).

When we implement the control unit, we will have to impement multiplexors wheneve rwe identify that we need to 
select one input out of many. For example, if we know that there are multiple possible inputs in the 

Let's think about how different instruction types are implemented, or what steps they go through:

1. R-type instructions
\begin{enumerate}
		\item{\textit{load value}}: First, we would load the instruction that is specified by the PC.
		\item{\textit{decode, increse PC}}: Next, we pass the rs,rd,and rt values to the register file.
				Meanwhile, we add 4 to the PC and make that the address of our next isntruction. We also 
				check the opcode and funct values and pass that as ALUsrc.
		\item{\textit{carry out the calculation}}: Based on that info, we already have the proper ALU control 
				input, so we pass the rs and rt to the main ALU, and with the ALU control input, carry out
				the desired operation.
		\item{\textit{store value in register}}: Since we are not writing to memory, the write enable will be 
				disabled, and we skip the memory area entirely, and write the result to the specified register
				in the register file.
\end{enumerate}
 
 \textit{REMEMER}: The wire ALUSrc determines if we will use the the value in the register as the second input 
 or if we will use the address and preform some arithmetic on it. This is independent from the ALUOp, which
 feeds the conrol unit and lets it know what to do.

 For a load instruction,which again is of the form \texttt{lw \$t1, offset(\$ t2)} , we can think of the order of execution as :
 2. Load/store instructions
 \begin{enumerate}
		 \item{\textit{Fetch instruction, increment PC}}: Same as before, we don't need to use the memory address
				 as the next PC, so we can directly increment by 4.
		 \item{\textit{Add offset value to base address}}: Since in the instruction we specify that the value
				 to be loaded as an offset in the register, we add them in the ALU. However, we first need
				 to shift the base address by 2 and sign-extend the rest. This is the final value we add to
				 the base address.
		 \item{\textit{Write value to register}}: When we finally add the offset to the base address, we need
				 to write the value to the register in the register file, since we would like to use it
				 in later calculations.
 \end{enumerate}
Lastly, for the \texttt{beq} isntruction, which is of the form \texttt{beq \$t1, \$t2, offset}, we perform 
the instructions in a simiar manner to R-Type instructions except with the result of the ALU operation.
\begin{enumerate}
		\item{\textit{Fetch the instruction, increment PC}}: We always fetch the instruction and increment the
				PC, but for branch instructions we need to determine which addresses we will use.
		\item{\item{Subtract the two numbers}}: Again with the sign=extended sign bits, we check the difference
				of the two numbers. If they are equal, the \texttt{ZERO} otuput wire will be asserted. If the 
				branch wire is also asserted from the Control unit, then we will use the target address
				specified in the instruction as our new PC address.
\end{enumerate}
For the beq instruction, we don't use any elements from the data memory part of the unit.

If we try to implement jump instructions, the main difference is that we compute the PC in a different way. We still
add 4 to the orginal PC and then we concatenate. The things we do to calculate the (new)PC: take the most 
significant 4 bits of the PC, the next 26 bits are the immediate bits, and add two 0s at the end (remember that
to indicate to jump a certain number of \textit{words} instead of bytes, we multiply by 4).

Implementing an instruction set where every instruction takes the same amount of time is inefficient because
the longest instruction will determine the clock time. Some operations don't use all of the modules in our
diagram, so it would be unnecessary for them to take the same amount of time as a longer instruction.
\subsection{4.5 An voerview of Pipelining}
To understand pipelining, we can think of a process repeating the instructions. If we have to wait for an entire
instruction to finish, then we would be wasting a large amount of time. The idea of \textit{\textbf{pipelinging}}
comes in. Pipelining is the idea of executing multiple insrtuctions at the same time. Pipelining does not 
make the execution of a single instruction faster, rather it makes the amount of instructions executed per 
unit time more.
As a reminder, the MIPS instruction set traditionally has five instruction stages:
\begin{enumerate}
		\item{Fetching the instruction from memory.}
		\item{Locate the required registers in register file, decode the instruction}
		\item{Actually operate on the instructions in the ALU}
		\item{access the data memeber in data memory}
		\item{write the instruction to memory}
\end{enumerate}
After we fetch the instruction for the first instruction, we can in theory fetch the next instruction in memory and
so on. Thus we always have the stages in the pipeline operating on some operands or another.

All stages in the pipeline take a single clock cycle, so the clock time should be enough to accomodate the slowest
stage in the pipeline individually, rather than the entire instruction as a whole.

Immeditely after we fetch the isnruction using the PC, we can move on to the next instruction. This means that
the time we need to wait between insructions is just equal to the time of the first stage, rather than the sum
of all the stages. The improvement with pipelining would be approximately equal to:
\begin{equation}
		total\_time=time\_between\_instructions * number\_of\_instructions
\end{equation}
Processing the MIPS instruction set is considerably simpler than x86 because the instructions are all the same 
length and the instruction format has some things in common accross all different instruction types. 
\subsubsection{Pipeline hazards}
A \textit{\textbf{structural hazard}} would refer to the inability of the hardware to support a certain
combination og instructions.

There is also the idea of \textit{\textbf{data hazards}}, where an instruction cannot be executed in a 
single cycle because the data which it requires is not yet available for use. For example, if we try to 
use the result of an arithmetic operation in the instruciton immediately following it we might have a 
data hazard because the data won't be written into its appropriate register until the last stage, which
is after the stage where we would need it in the next instruction.

How can we deal with the data hazards? One way would be to have the result from the ALU directly go to the next
step. We could store it ina temporary buffer and then make it available to the next instruction. This is called
\textit{\textbf{forwarding}} or \textit{\textbf{bypassing}}.

If we did not have the abiltiy to forward or bypass, we would need to stall the pipeline until all the operations
were completed, so if there was a data hazard we would need to make the entire rest of the pipeline wait. Because
we still can't go back in time, we could onlybe able to forward information to future stages of the pipeline. For
example, we would not be able to send the result of a memory instruction (such as load) to the ALU, since that 
stage happens before we have the final address. An R-type instruction immediately following a load instruction
would definitely require stalling at least until the fourth stage in the pipeline. This specific type of 
data hazard is called \textit{\textbf{load-use data hazard}}.

A \textit{\textbf{pipeline stall}} is the process of stopping the pipeline to resolve a data hazard.

The third type of hazard is called \textit{\textbf{control hazard}}. A structural hazard comes arises
when the  current instruction depends on the outcome of instructions that are still executing. For
example, the outcome address of a branch address might not be calculated until the fourth stage, and 
written until the 5th, so the next operation would not be able to fetch the instruction address
in the \textit{instruction fetch} stage. Control hazards occur with branch instructions.

One option would be to immediately stall upon receiving a branch instruction. Why do they happen though?
They occur because after we fetch the instruction for the current instruction (the branch instruction)
, we start fetching the address of the next instruction. At this moment, we cannot possibly know the
address of the next instruction since we just started calling the registers from memory.

We can try to \textit{predict} the result of a branch before it happens. What is the cost? Well, if
you guess correctly, then there would be no cost, since the pipeline would operate as if nothing 
happened. If you were wrong, you mgiht have to stall. A simple approach 
would be to assume that the branch will be untaken (that there will be no branch).

Therefore, \textit{\textbf{branch prediction}} is the practice of assuming a certain outcome of the
instruction and proceed from that instead of actually waiting for the entire procedure. To do this,
we could have rigid measures such as always assuming branches at the end of loops that go back 
to an earlier address (beginning of loop) are always taken, however this doens't allow for
much flexibility. 

There can also be \textit{dynamic predictors}, which keep a history of recent branches and 
whether they were taken or not taken. Then, it can adjust its assumption based on current
behavior. The accuracy of these predictors can reach levels 90\%<.

As the last part to the section. There can also be \textit{delayed branches}. When we 
encounter a branch, we can stall until the instruction finishes calculating the next
address. In the meantime, because there is at least one clock cycle where the pipeline
would be stalled, we can instead insert another insrtuction that does not depend on
the branch.

\hspace{10mm}\textbf{Summary of pipelining}
With pipelinging, we don't increase the speed of an individual instruction, but we 
increase the amount of instructions executed in a given amount of time. This helps
increase the \textit{throughput} of the program. There is also the idea of latency, 
or the amount of stages between two consectutive instructions in the pipeline.
\subsection{4.6 Pipelined Datapath and Control}
Given the diagram for the Datapath, we can split it into the 5 stages. All the 
operations move the information 'forward', or to the next area in the cycle, 
except for updating the PC and writing the result of calculations into the 
register file.

The figure in the book shows the intruction datapath divided into the segments. In order to be
able to use and store the results after every stage, we insert a register after every output 
operation.

The main idea of this section relates to the fact that we can set multiple instructions to occur
simultaneously. This may raise some issues in the instructions that come after it.
\subsubsection{Pipelined Control}
For pipelining instructions, similar to the way we implemente a control unit for the simple 
datapath, we would need a control unti for the pipelining. Since every stage is active during 
every clock cycle, we need to have conrol wires for the stages in the pipeline where we want 
to write. For this datapath, we could split the control wires into 5 groups:
\begin{enumerate}
		\item{\textit{\textbf{Instruction Fetch}}}: Since in every instruction we operate on
				the PC and retrieve the current instruction address, any control wires to the IF
				stage would always be asserted.
		\item{\textbf{\textit{Instruction decode/register file read}}}: Again, because we use these
				instructions in every time, then these wires will always be asserted.
\end{enumerate}
\newpage
\subsection{Data Hazards: Forwarding versus stalling}
Again, when we have values that have to be used by other instructions, such as the result of
arithmetic operations, we might have a \textit{data hazard}, since the value we need in the 
next instruction won't be available until after the ALU operation is supposed to take place.

The idea qould be to send the values to the next instruction if the result from one stage is needed 
in the calculation of another stage. We would need to check that the operand registers in the next
instruction are not the same ones as the result register in after the EX stage.

We would also need to be able to detect a potential hazard if it could occur. How could we do it? 
Well, we could use the control signal for the registers. Also, we would need to check the state 
of the instructions to see if a hazard could occur. For example, if we have a load instruction 
followed by a R-type instruction, the value won't be written into the output register until after the 
next instruciton needs it for its EX stage. Thus, we would need some mechanism to stall the 
pipeline. We could do this by having the pipeline \textit{not} change the PC, and by setting all the
control signals to 0. This would mean that the state would not change, as well as the instruction.
The current PC value would continue to be read 

This has the effect of delaying all instructions by at least one clock cycle, until it can resolbe all conflicts.
When would we use register forwarding versus stalling? Well, we can use forwarding when the result of one
operation is used as the input for the next register. However, we cannot forward when there is a load
instruction and a logic operation in the next instruction. In the latter case, when there is the risk
of a \textit{hazard}, we would stall the pipeline by adding a bubble, or deactivating the control 
signals and not changing the PC register. More specifically, we would need to stall the pipeline when 
the following instruction reads the value that was just loaded into the register.

\subsection{Control Hazards}
As we saw previously, the type of hazard that can occur when there is a branch instruction is called a
\textit{control hazard}, or a type of hazard in which we have a branch instruction, but we don't know
if we have to branch or not until the MEM stage, at which point the next couple instructions have
already started to begin calculation. The ways to deal with it are not as effective as the 
ways to deal with data hazards. There are some ways to deal with this type of incident: assuming
the branch is not taken and dynamic branch prediction.

\hspace{10mm}\textbf{Assume the Branch is not taken}
When we assume the branch is not taken, the instructions following the branch will have to exeute normally. 
At the end of the MEM stage, we will know if the branch is taken or not. If it is, we then will have to
\textit{flush} the instructions following the branch.

Making the branch decision faster is also possible under this scheme, since the branch address can be moved
up. The branch conditions are usually simple ones, so with a few logic gates we could calculate the 
result of the simple branches. However, to implement this type of change, we would need to first need to XOR
the resiters and then 'OR al the results'(?). However, we would need to change the forwarding logic introduced
for the ALU stage previously. If the previous ALU calculation produced one of the operands for the current 
instruction, then a stall would be needed. However, if a load is needed right before a branch instruction,
up to 2 stalls might be needed.

Even with all its difficulties, moving the branch to the ID stage is stll an improvement, since we could 
know the result of a branch and only have to (potentially) flush the instruction currently being fetched.

\hspace{10mm}\textbf{Dynamic Branch Prediction}
Dynamic branch prediction refers to the decision of whether or not to branch at runtime. As the pipeline 
gets more involved, we would lose too much performance implementing a simple branch prediction scheme.
The way we implement this type of prediciton is by keeping some sort of record, called a 
\textit{branch history table}, in which we keep a record of whether the instruction's branch was taken 
last time it was executed. The table is indexed by the lower order bits of the instruction, which 
might present an instruction if two instructions with the same lower order bits are indexed in the table.

As an example, in a 1-bit prediction scheme, the first and last predictions in a series of the same instructions
will be wrong. Why? Well, suppose the instruction is taken 9 times in a row, and not taken the last time. In
the table, the entry will be set to 'not taken' from the previous time it was not taken. Then it will be 
predicted the next 8 times correctly and incorrectly the very last time , when we predict the branch 
to be taken and it isnot taken. Thus the accuracy of this scheme will always fail on the first and
last tries. How to remedy this?  We can use 2 bits to make the prediction instead of 1. Then , we only change
the prediction in the table once our prediction has been wrong twice.

To avoid the branch delay slot, which is due to the time it takes to calculate the PC, we can instead store
the target PC in a \textit{branch target buffer}.

In order to make the predictions more accurate, we can use \textit{correalting predictors}. These are 
other additional bits of global information. We can use these bits to increase the branch prediction
accuracy becasue it serves as adding an extra idnex bit for each branch lookup.

A newer method for doing prediction involves using \textit{tournament predictors}. These predictors will 
use different predictors for each branch then choose from the ones that have the best results.
\subsection{Exceptions}
An \textit{exception} is an unexpected event (such as overflow) that may arise during the execution of a 
program. Exceptions may be caused from within the program itself. An \textit{interrupt} is an exception
that comes from outside the program.

The main thing that needs to happen when an exception occurs is that we need to first detect the 
exception obviously, then we need to save the address of the instruction that caused the exception in 
the \textit{Exception Program Counter}, then finally return control to the operating system for it to 
decide how to deal with it. We also would need to specify the reason for the interrupt, which we can do 
by placing a specific field in the \textit{Cause Register}, or we could use a 
\textbf{\textit{vectored interrupt }}. A vectored interrupt specifies the address to which the control is 
returned based on the reason for te exception. Based on the adress of the register on which we are 
storing the values, the operating system can determine the cause of the exception. 

How can we detectexceptions? We can think of them as anothe type of control hazard. If voerflow occurs,
for example, we can flush the next instructions and then insert nops.
\end{document}

